{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from utils import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsi_np, wlens = LoadHSI('../Data/cropped_hdf5/FX10_09SEPT2023_3D1_0.hdf5', return_wlens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_np = read_mask('../Data/cropped_masks/FX10_09SEPT2023_3D1_0.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining image paths for healthy, early-, mid- and late-diseased leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_filenames(folder_path, camera_id, date_stamps, tray_ids):\n",
    "    \"\"\"\n",
    "    Filters filenames in a folder based on camera ID, date stamps, and tray IDs.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing the files.\n",
    "    - camera_id (str): Camera ID.\n",
    "    - date_stamps (list): List of selected date stamps.\n",
    "    - tray_ids (list): List of selected tray IDs.\n",
    "\n",
    "    Returns:\n",
    "    - list: Filtered list of full file paths that match the given criteria.\n",
    "    \"\"\"\n",
    "    all_files = os.listdir(folder_path)\n",
    "\n",
    "    filtered_files = [\n",
    "        os.path.join(folder_path, f) for f in all_files\n",
    "        if f.startswith(camera_id + \"_\") and \n",
    "           any(date in f for date in date_stamps) and \n",
    "           any(f.split(\"_\")[2].startswith(tray) for tray in tray_ids)\n",
    "    ]\n",
    "    \n",
    "    return filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FX10 camera\n",
    "IMG_DIR = '../Data/cropped_hdf5'\n",
    "CAMERA = 'FX10'\n",
    "\n",
    "# Healthy leaves\n",
    "DATES = ['07SEPT2023', '08SEPT2023', '09SEPT2023', '10SEPT2023', '11SEPT2023', '12SEPT2023',\n",
    "         '13SEPT2023', '14SEPT2023', '15SEPT2023', '18SEPT2023', '19SEPT2023']\n",
    "TRAYS = ['3D', '4C', '4D', '2D']    # Some files from the FX17 camera are mistakenly named in 2D instead of 4D\n",
    "healthy_FX10 = filter_filenames(folder_path=IMG_DIR, camera_id=CAMERA, date_stamps=DATES, tray_ids=TRAYS)\n",
    "\n",
    "# Early diseased leaves\n",
    "DATES = ['07SEPT2023']\n",
    "TRAYS = ['3C']\n",
    "early_diseased_FX10 = filter_filenames(folder_path=IMG_DIR, camera_id=CAMERA, date_stamps=DATES, tray_ids=TRAYS)\n",
    "\n",
    "# Mid diseased leaves\n",
    "DATES = ['08SEPT2023', '09SEPT2023']\n",
    "TRAYS = ['3C']\n",
    "mid_diseased_FX10 = filter_filenames(folder_path=IMG_DIR, camera_id=CAMERA, date_stamps=DATES, tray_ids=TRAYS)\n",
    "\n",
    "# Late diseased leaves\n",
    "DATES = ['10SEPT2023', '11SEPT2023', '12SEPT2023', '13SEPT2023', '14SEPT2023', '15SEPT2023']\n",
    "TRAYS = ['3C']\n",
    "late_diseased_FX10 = filter_filenames(folder_path=IMG_DIR, camera_id=CAMERA, date_stamps=DATES, tray_ids=TRAYS)\n",
    "\n",
    "# Number of samples in each category\n",
    "print(f\"Healthy: {len(healthy_FX10)}\")\n",
    "print(f\"Early diseased: {len(early_diseased_FX10)}\")\n",
    "print(f\"Mid diseased: {len(mid_diseased_FX10)}\")\n",
    "print(f\"Late diseased: {len(late_diseased_FX10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 12 images from healthy, mid-and late-diseased leaves\n",
    "random.seed(10)\n",
    "healthy_sample = random.sample(healthy_FX10, 12)\n",
    "mid_diseased_sample = random.sample(mid_diseased_FX10, 12)\n",
    "late_diseased_sample = random.sample(late_diseased_FX10, 12)\n",
    "\n",
    "print(healthy_sample)\n",
    "print(mid_diseased_sample)\n",
    "print(late_diseased_sample)\n",
    "\n",
    "# Concatenate the lists\n",
    "pca_data = healthy_sample + early_diseased_FX10 + mid_diseased_sample + late_diseased_sample\n",
    "print(len(pca_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(hsi_np, wlens, min_wavelength=0, normalize=True):\n",
    "    \"\"\"\n",
    "    Removes spectral bands with wavelengths below min_wavelength from the hyperspectral image.\n",
    "    Also replaces negative values with 0 and applies optional normalization.\n",
    "\n",
    "    Parameters:\n",
    "    - hsi_np (nunmpy array): Hyperspectral image with shape (bands, height, width).\n",
    "    - wlens (numpy array): Wavelength values with shape (bands, ) corresponding to bands in hsi_np\n",
    "    - min_wavelength (int or float): Minimum wavelength (in nm) to keep in the hyperspectral data\n",
    "    - normalize (bool): Whether to normalize the hyperspectral image data\n",
    "\n",
    "    Returns:\n",
    "    - hsi_np_filtered (numpy array): Hyperspectral image with selected spectral bands of shape (filtered bands, height, width)\n",
    "    - wlens_filtered (numpy array): Updated wavelengths array of shape (filtered bands, )\n",
    "    \"\"\"\n",
    "    # Determine wavelengths to keep\n",
    "    valid_bands = wlens >= min_wavelength  \n",
    "\n",
    "    # Filter hsi_np and wlens to keep only relevant bands\n",
    "    hsi_np_filtered = hsi_np[valid_bands, :, :]\n",
    "    wlens_filtered = wlens[valid_bands]\n",
    "    \n",
    "    # Set all negative values to 0 (these are noise)\n",
    "    hsi_np_filtered = np.maximum(hsi_np_filtered, 0)\n",
    "    \n",
    "    # Normalize the data if required\n",
    "    if normalize and np.max(hsi_np_filtered) > 0:    # Avoid division by zero\n",
    "        hsi_np_filtered = hsi_np_filtered / np.max(hsi_np_filtered)\n",
    "    \n",
    "    return hsi_np_filtered, wlens_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_flatten_hsi(img_paths, mask_dir=None, individual_normalize=False, apply_mask=False, mask_method=1, min_wavelength=0):\n",
    "    \"\"\"\n",
    "    Transforms the 3D hyperspectral images into a 2D array by flattening the spatial dimensions.\n",
    "    The resulting \"rows\" are the pixels and the \"columns\" store their values for the different spectral bands. \n",
    "    Can be used with a single- or multiple HSI-s. If multiple HSI-s are provided, they are stacked together.\n",
    "\n",
    "    Parameters:\n",
    "    - img_paths (list of str): Paths to the HSI files to be loaded and flattened\n",
    "    - mask_dir (str): Path to the folder containing the masks for the HSI-s\n",
    "    - individual_normalize (bool): Whether to normalize each HSI individually before flattening and stacking\n",
    "    - apply_mask (bool): Whether to apply the mask to the HSI-s\n",
    "    - mask_method (int): 0 for keeping only leaf, 1 for keeping leaf+stem after applying the mask\n",
    "    - min_wavelength (int or float): Minimum wavelength (in nm) to keep in the hyperspectral data\n",
    "\n",
    "    Returns:\n",
    "    - numpy array, shape (n_pixels, n_bands), the flattened and stacked HSI pixels\n",
    "    \"\"\"\n",
    "    all_pixels = []\n",
    "    \n",
    "    # Load wavelengths from the first image (but could be any image since images have the same spectral bands)\n",
    "    _, wlens = LoadHSI(img_paths[0], return_wlens=True)\n",
    "    \n",
    "    for file in img_paths:\n",
    "        hsi_np = LoadHSI(file)\n",
    "        \n",
    "        # Load and apply mask if required\n",
    "        if apply_mask and mask_dir:\n",
    "            mask_file = os.path.join(mask_dir, os.path.basename(file).replace('.hdf5', '.png'))    # Find the mask for the HSI (same name)\n",
    "            mask_np = read_mask(mask_file)\n",
    "            hsi_np = hsi_np * np.where(mask_np == 2, mask_method, mask_np)\n",
    "        \n",
    "        # Preprocess the hyperspectral image   \n",
    "        hsi_np, _ = preprocess(hsi_np, wlens, min_wavelength=min_wavelength, normalize=individual_normalize)\n",
    "\n",
    "        # Flatten: (bands, height, width) â†’ (height*width, bands)\n",
    "        hsi_np = hsi_np.reshape(hsi_np.shape[0], -1).T\n",
    "        \n",
    "        # Remove background (zero) pixels (those that were masked out)\n",
    "        if apply_mask:\n",
    "            hsi_np = hsi_np[~np.all(hsi_np == 0, axis=1)]\n",
    "\n",
    "        all_pixels.append(hsi_np)\n",
    "    \n",
    "    # Stack all pixels together\n",
    "    return np.vstack(all_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "X = load_and_flatten_hsi(img_paths=pca_data, mask_dir='../Data/cropped_masks',\n",
    "                         apply_mask=True, individual_normalize=True, mask_method=0, min_wavelength=430)\n",
    "print(f\"Data shape before PCA: {X.shape}\")\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=30)    # Keep 30 components\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Print results\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Number of components chosen: {pca.n_components_}\")\n",
    "print(f\"Explained variance ratio: {cumulative_variance_ratio[-1]:.4f}\")\n",
    "print(f\"Data shape after PCA: {X_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o', label='Individual Variance')\n",
    "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='s', label='Cumulative Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance Explained')  # Optional: mark the 95% line\n",
    "plt.xlabel('Principal Component Number')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loadings\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "# Plot PC loadings\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(pca.n_components_):\n",
    "    plt.plot(wlens, loadings[:, i], label=f'PC {i+1}')\n",
    "\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Loading Value')\n",
    "plt.title('Principal Component Loadings')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def plot_spectra(X, wlens):\n",
    "    # Initialize a list to store spectral values for averaging\n",
    "    all_spectral_values = []\n",
    "    \n",
    "    # Plot the spectral values for the pixels\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(len(X)):\n",
    "        all_spectral_values.append(X[i])  # Collect spectral values\n",
    "        plt.plot(wlens, X[i], color='lightblue', linewidth=0.4, alpha=0.3)  # Blue color with reduced transparency\n",
    "        \n",
    "    # Convert the list to a numpy array for easier statistical computation\n",
    "    all_spectral_values = np.array(all_spectral_values)\n",
    "    \n",
    "    # Compute the mean and standard deviation\n",
    "    mean_spectral_values = np.mean(all_spectral_values, axis=0)\n",
    "    std_spectral_values = np.std(all_spectral_values, axis=0)\n",
    "    \n",
    "    # Plot the mean spectral values as a thick dark blue line\n",
    "    plt.plot(wlens, mean_spectral_values, color='darkblue', linewidth=3, label='Mean Spectral Value')\n",
    "    \n",
    "    # Plot the standard deviation as shaded areas around the mean (dark blue with transparency)\n",
    "    plt.fill_between(\n",
    "        wlens, \n",
    "        mean_spectral_values - std_spectral_values, \n",
    "        mean_spectral_values + std_spectral_values, \n",
    "        color='darkblue', alpha=0.5, label='Â±1 Std Dev'\n",
    "    )\n",
    "    \n",
    "    # Customize ticks on both axes\n",
    "    ax = plt.gca()\n",
    "    # X-axis ticks\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(100))  # Big ticks every 100\n",
    "    ax.xaxis.set_minor_locator(ticker.MultipleLocator(10))   # Small ticks every 20\n",
    "    # Y-axis ticks\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(5000))  # Big ticks every 5000\n",
    "    ax.yaxis.set_minor_locator(ticker.MultipleLocator(1000)) # Small ticks every 1000\n",
    "\n",
    "    # Add titles, labels, and legend\n",
    "    plt.xlabel('Wavelength (nm)')\n",
    "    plt.ylabel('Reflectance')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct PCA\n",
    "X_reconstructed = pca.inverse_transform(X_pca)\n",
    "X_reconstructed = scaler.inverse_transform(X_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_pca.shape)\n",
    "print(X_reconstructed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "plot_spectra(X[5000:10000], wlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructed\n",
    "plot_spectra(X_reconstructed[5000:10000], wlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsi_transform_to_pca_space(hsi_np, pca, scaler, mask_np=None, mask_method=1):\n",
    "    \"\"\"\n",
    "    Apply pre-trained PCA on HSI to reduce spectral bands, i.e. transform data to PCA space.\n",
    "    If a mask is provided, only the valid non-background pixels are transformed to the PCA space with the pre-trained PCA model. Also,\n",
    "    the background pixels will be set to 0 in the PCA space as well.\n",
    "\n",
    "    Parameters:\n",
    "    - hsi_np (numpy array): Hyperspectral image with shape (bands, height, width).\n",
    "    - pca: Pre-fitted PCA model.\n",
    "    - scaler: Pre-fitted StandardScaler model.\n",
    "    - mask_np (numpy array): Mask to apply on the HSI.\n",
    "    - mask_method (int): 0 for keeping only leaf, 1 for keeping leaf+stem after applying the mask.\n",
    "\n",
    "    Returns:\n",
    "    - numpy array of PCA-transformed HSI with shape (pca_components, height, width).\n",
    "    \"\"\"\n",
    "    # Set all negative values to 0 (these are noise)\n",
    "    # hsi_np = np.maximum(hsi_np, 0)    # Q: Interestingly if we do this, the reconstruction errors show vertical lines\n",
    "    # A: Because we would need to change negative values to 0 in the function that calculates reconstruction error as well to the\n",
    "    # \"original\" hsi_np that we compare the reconstructed to. â†’ Better apply np.maximum(hsi_np, 0) before calling the function and not inside it.\n",
    "    \n",
    "    # Flatten: (bands, height, width) â†’ (height*width, bands)\n",
    "    hsi_flattened = hsi_np.reshape(hsi_np.shape[0], -1).T\n",
    "    \n",
    "    # Apply mask if provided (exclude background from PCA)\n",
    "    if mask_np is not None:\n",
    "        mask_np = np.where(mask_np == 2, mask_method, mask_np)\n",
    "        valid_indices = mask_np.flatten() != 0\n",
    "        hsi_valid = hsi_flattened[valid_indices]    # Keep only non-background pixels\n",
    "    else:\n",
    "        hsi_valid = hsi_flattened\n",
    "\n",
    "    # Standardize using the previously fitted scaler\n",
    "    hsi_valid_scaled = scaler.transform(hsi_valid)\n",
    "\n",
    "    # Apply the pre-trained PCA model\n",
    "    hsi_pca_valid = pca.transform(hsi_valid_scaled)\n",
    "\n",
    "    # Return to the original number of pixels, with 0s for background pixels if mask was applied and PCA-transformed pixels for the rest\n",
    "    if mask_np is not None:\n",
    "        hsi_pca = np.zeros((hsi_flattened.shape[0], pca.n_components_))    # Initialize empty array with the shape of the original pixels\n",
    "        hsi_pca[valid_indices] = hsi_pca_valid    # Insert only valid transformed pixels\n",
    "    else:\n",
    "        hsi_pca = hsi_pca_valid    # No mask applied, just return transformed pixels\n",
    "\n",
    "    # Reshape back to (pca_components, height, width)\n",
    "    return hsi_pca.T.reshape(pca.n_components_, hsi_np.shape[1], hsi_np.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_hsi_from_pca_space(hsi_pca, pca, scaler, mask_np=None, mask_method=1):\n",
    "    \"\"\"\n",
    "    Back-transform PCA-transformed HSI to original space.\n",
    "    If a mask is provided, only the valid non-background pixels are reconstructed from the PCA space to the original space, with background\n",
    "    pixels set to 0.\n",
    "    \n",
    "    Parameters:\n",
    "    - hsi_pca (numpy array): PCA-transformed HSI with shape (pca_components, height, width).\n",
    "    - pca: Pre-fitted PCA model.\n",
    "    - scaler: Pre-fitted StandardScaler model.\n",
    "    - mask_np (numpy array): Mask to apply on the HSI.\n",
    "    - mask_method (int): 0 for keeping only leaf, 1 for keeping leaf+stem after applying the mask.\n",
    "\n",
    "    Returns:\n",
    "    - numpy array of back-transformed (reconstructed) HSI with shape (bands, height, width).\n",
    "    \"\"\"\n",
    "    # Flatten: (pca_components, height, width) â†’ (height*width, pca_components)\n",
    "    hsi_pca_flattened = hsi_pca.reshape(pca.n_components_, -1).T\n",
    "    \n",
    "    # Apply mask if provided (exclude background from PCA)\n",
    "    if mask_np is not None:\n",
    "        mask_np = np.where(mask_np == 2, mask_method, mask_np)\n",
    "        valid_indices = mask_np.flatten() != 0\n",
    "        hsi_valid = hsi_pca_flattened[valid_indices]    # Keep only non-background pixels\n",
    "    else:\n",
    "        hsi_valid = hsi_pca_flattened\n",
    "\n",
    "    # Apply inverse PCA\n",
    "    hsi_valid_reconstructed = pca.inverse_transform(hsi_valid)\n",
    "\n",
    "    # Apply inverse scaling\n",
    "    hsi_valid_reconstructed = scaler.inverse_transform(hsi_valid_reconstructed)\n",
    "    \n",
    "    # Reconstruct full spatial structure if mask was applied\n",
    "    if mask_np is not None:\n",
    "        hsi_reconstructed = np.zeros((hsi_pca_flattened.shape[0], hsi_valid_reconstructed.shape[1]))    # Initialize empty array of shape (original pixels, original bands)\n",
    "        hsi_reconstructed[valid_indices] = hsi_valid_reconstructed    # Insert only valid transformed pixels\n",
    "    else:\n",
    "        hsi_reconstructed = hsi_valid_reconstructed    # No mask applied, just return reconstructed pixels\n",
    "\n",
    "    # Reshape back to (bands, height, width)\n",
    "    return hsi_reconstructed.T.reshape(-1, hsi_pca.shape[1], hsi_pca.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_and_reconstruct_hsi_pca(hsi_np, pca, scaler, mask_np=None, mask_method=1):\n",
    "    '''\n",
    "    Preform PCA compression and reconstruction right after on a HSI data.\n",
    "    \n",
    "    Parameters:\n",
    "    - hsi_np (numpy array): Hyperspectral image with shape (bands, height, width).\n",
    "    - pca: Pre-fitted PCA model.\n",
    "    - scaler: Pre-fitted StandardScaler model.\n",
    "    - mask_np (numpy array): Mask to apply on the HSI.\n",
    "    - mask_method (int): 0 for keeping only leaf, 1 for keeping leaf+stem after applying the mask.    \n",
    "    '''\n",
    "    # Transform data to PCA space\n",
    "    hsi_pca = hsi_transform_to_pca_space(hsi_np, pca, scaler, mask_np, mask_method)\n",
    "    \n",
    "    # Reconstruct data from PCA space\n",
    "    hsi_reconstructed = reconstruct_hsi_from_pca_space(hsi_pca, pca, scaler, mask_np, mask_method)\n",
    "    \n",
    "    return hsi_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_reconstruction_error(hsi_np, pca, scaler, mask_np=None, mask_method=1, show_plot=True):\n",
    "    \"\"\"\n",
    "    Reconstruct an input HSI using the pre-trained PCA, calculate the reconstruction error and optionally plot the sum of it across bands.\n",
    "    Should be used with a single HSI file.\n",
    "\n",
    "    Parameters:\n",
    "    - hsi_np (numpy array): Hyperspectral image with shape (bands, height, width)\n",
    "    - pca: pre-fitted PCA model\n",
    "    - scaler: pre-fitted StandardScaler model\n",
    "    - mask_np (numpy array): Mask to apply on the HSI\n",
    "    - mask_method (int): 0 for keeping only leaf, 1 for keeping leaf+stem after applying the mask\n",
    "    - show_plot (bool): Whether to show the plot of the sum of reconstruction errors across the bands\n",
    "\n",
    "    Returns:\n",
    "    - reconstruction_error (numpy array): The reconstruction error of the HSI (for each band) with shape (bands, height, width) \n",
    "    - Plot (optional): \"Heatmap\" of the sum of reconstruction errors across the bands\n",
    "    \"\"\"\n",
    "    # Apply PCA and reconstruct\n",
    "    hsi_pca = hsi_transform_to_pca_space(hsi_np, pca, scaler, mask_np, mask_method)\n",
    "    hsi_reconstructed = reconstruct_hsi_from_pca_space(hsi_pca, pca, scaler, mask_np, mask_method)\n",
    "    \n",
    "    # Compute reconstruction error\n",
    "    if mask_np is not None:\n",
    "        mask_np = np.where(mask_np == 2, mask_method, mask_np)\n",
    "        reconstruction_error = np.abs(hsi_np - hsi_reconstructed) * mask_np\n",
    "    else:\n",
    "        reconstruction_error = np.abs(hsi_np - hsi_reconstructed)\n",
    "        \n",
    "    if show_plot:\n",
    "        # Sum reconstruction error across the bands\n",
    "        pixel_errors = np.sum(reconstruction_error, axis=0)\n",
    "\n",
    "        # Plot pixel errors\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(pixel_errors, cmap='hot')\n",
    "        plt.colorbar(label='Total Reconstruction Error')\n",
    "        plt.title('Total Reconstruction Error per Pixel')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    return reconstruction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_reconstruction_error_dir(hsi_path, pca, scaler, mask_dir=None, apply_mask=False, mask_method=1):\n",
    "    \"\"\"\n",
    "    Reconstruct an input HSI using the pre-trained PCA and plot the reconstruction error.\n",
    "    Should be used with a single HSI file.\n",
    "    Since the hsi_np is not fed to the function, but it is loaded inside the function, we can't np.max(hsi_np, 0) to set negative values to 0,\n",
    "    which is a bit undesirable, so this function is meant to used mainly for quick experimentation purposes.\n",
    "    (Well, we could np.max(hsi_np, 0) inside this function but then we would also need that in the hsi_transform_to_pca_space() function.)\n",
    "\n",
    "    Parameters:\n",
    "    - hsi_path (str): Path to the HSI file.\n",
    "    - pca: Pre-fitted PCA model.\n",
    "    - scaler: Pre-fitted StandardScaler model.\n",
    "    - mask_dir (str): Path to the folder containing the masks for the HSI-s.\n",
    "    - apply_mask (bool): Whether to apply the mask to the HSI-s.\n",
    "    - mask_method (int): 0 for keeping only leaf, 1 for keeping leaf+stem after applying the mask.\n",
    "\n",
    "    Returns:\n",
    "    - Plot (\"heatmap\") of the sum of reconstruction errors across the bands\n",
    "    \"\"\"\n",
    "    # Load a HSI\n",
    "    hsi_np = LoadHSI(hsi_path)\n",
    "    \n",
    "    # Load mask if required\n",
    "    if apply_mask and mask_dir:\n",
    "        mask_file = os.path.join(mask_dir, os.path.splitext(os.path.basename(hsi_path))[0] + \".png\")    # Find the mask for the HSI (same name)\n",
    "        mask_np = read_mask(mask_file)\n",
    "        mask_np = np.where(mask_np == 2, mask_method, mask_np)\n",
    "        \n",
    "    # Apply PCA and reconstruct\n",
    "    hsi_reconstructed = compress_and_reconstruct_hsi_pca(hsi_np, pca, scaler, mask_np, mask_method)\n",
    "    \n",
    "    # Compute reconstruction error\n",
    "    if apply_mask and mask_dir:\n",
    "        reconstruction_error = np.abs(hsi_np - hsi_reconstructed) * mask_np\n",
    "    else:\n",
    "        reconstruction_error = np.abs(hsi_np - hsi_reconstructed) \n",
    "    \n",
    "    # Sum reconstruction error across the bands\n",
    "    pixel_errors = np.sum(reconstruction_error, axis=0)\n",
    "\n",
    "    # Plot pixel errors\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(pixel_errors, cmap='hot')\n",
    "    plt.colorbar(label='Total Reconstruction Error')\n",
    "    plt.title('Total Reconstruction Error per Pixel')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_error = get_pca_reconstruction_error(hsi_np, pca, scaler, mask_np, mask_method=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_reconstruction_error_dir('../Data/HDF5_FILES/train/FX10_07SEPT2023_1B1.hdf5', pca, scaler\n",
    "                              , mask_dir='../Data/MASKS/train', apply_mask=True, mask_method=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize original img and img in PC space (first 3 bands/PCs) and reconstructed img (with 3 selected \"RGB\" bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsi_np = np.maximum(hsi_np, 0)\n",
    "# hsi_np = hsi_np  / np.max(hsi_np)    # Could add this as well but then pca would need to be trained with individual_normalize=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsi_reconstructed = compress_and_reconstruct_hsi_pca(hsi_np, pca, scaler, mask_np)\n",
    "hsi_pca = hsi_transform_to_pca_space(hsi_np, pca, scaler, mask_np=mask_np, mask_method=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we find the bands corresponding to the RGB channels\n",
    "RGB_wlens = (445, 535, 575)\n",
    "RGB_bands = np.argmin(np.abs(np.array(wlens)[:, np.newaxis] - RGB_wlens), axis=0)\n",
    "print(f'RGB indices in hsi -> {RGB_bands}')\n",
    "\n",
    "# we create and RGB image from the hsi by selecting those bands, \n",
    "# but first set all negative values to 0 (these are noise) and normalize the hsi\n",
    "# hsi_np = np.maximum(hsi_np, 0)\n",
    "hsi_np = hsi_np  / np.max(hsi_np)\n",
    "\n",
    "# The pca space can easily have negative values and positives larger that 255. The reconstructed images should have values between 0 and 1 in\n",
    "# case hsi_np was already between 0 and 1, although we can expect that reconstruction is not perfect and some values might be outside this range.\n",
    "# Clipping the values between 0 and 1 is not that bad though for VISUALIZATION purposes. For VISUALIZATION PURPOSES only, we may also clip the\n",
    "# values in the PCA space between 0 and 1. (This should not be done e.g. at calculating the reconstruction error.)\n",
    "hsi_pca = np.maximum(hsi_pca, 0)\n",
    "hsi_pca = hsi_pca / np.max(hsi_pca)\n",
    "\n",
    "hsi_reconstructed = np.maximum(hsi_reconstructed, 0)\n",
    "hsi_reconstructed = hsi_reconstructed  / np.max(hsi_reconstructed)\n",
    "\n",
    "# select the rgb bands\n",
    "rgb_img = hsi_np[RGB_bands,:,]\n",
    "hsi_pca_img = hsi_pca[0:3,:,]\n",
    "rgb_img_reconstructed = hsi_reconstructed[RGB_bands,:,]\n",
    "print(f'shape, {rgb_img.shape}, but for other libraries usually the bands is the last dimension, so we change the order and get:')\n",
    "\n",
    "rgb_img = rgb_img.transpose((1,2,0))\n",
    "hsi_pca_img = hsi_pca_img.transpose((1,2,0))\n",
    "rgb_img_reconstructed = rgb_img_reconstructed.transpose((1,2,0))\n",
    "print(rgb_img.shape)\n",
    "\n",
    "#  now we can visualize the image\n",
    "plt.figure(figsize = (16,50))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(rgb_img)\n",
    "plt.title('RGB')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(hsi_pca_img)\n",
    "plt.title('PCA reduced HSI')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(rgb_img_reconstructed)\n",
    "plt.title('Reconstructed RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe I saw somewhere that the PCA reduced HSI that is on the plot visualizes the parts with high(est) variance. It explains why we don't see a very clearly outlined image. We would need to add the average (or average + rescale?) to get a more meaningful image. Or something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsi_np.min(), hsi_np.max(), hsi_pca.min(), hsi_pca.max(), hsi_reconstructed.min(), hsi_reconstructed.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img.min(), rgb_img.max(), hsi_pca_img.min(), hsi_pca_img.max(), rgb_img_reconstructed.min(), rgb_img_reconstructed.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, the small \"dirt\" spot on the healthy leaf can be reconstructed with the false-RGB image, even though that part showed higher reconstruction errors. Probably other bands (not RGB) produce higher reconstruction errors there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm this, plot the reconstruction error for the RGB bands only and plot it for all bands\n",
    "Indeed seemingly the RGB bands don't produce high reconstruction errors at the \"dirt spot\".\n",
    "The reconstruction errors for the different bands show that there might be some bands that are more important (produce higher reconstruction error), which steers us towards band selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we find the bands corresponding to the RGB channels\n",
    "RGB_wlens = (445, 535, 575)\n",
    "RGB_bands = np.argmin(np.abs(np.array(wlens)[:, np.newaxis] - RGB_wlens), axis=0)\n",
    "print(f'RGB indices in hsi -> {RGB_bands}')\n",
    "\n",
    "# select RGB bands from reconstruction error\n",
    "reconstruction_error_rgb = reconstruction_error[RGB_bands,:,]\n",
    "print(reconstruction_error_rgb.shape)\n",
    "\n",
    "# Sum reconstruction error across the RGB bands\n",
    "pixel_errors = np.sum(reconstruction_error_rgb, axis=0)\n",
    "\n",
    "# Plot pixel errors\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(pixel_errors, cmap='hot')\n",
    "plt.colorbar(label='Total Reconstruction Error')\n",
    "plt.title('Total Reconstruction Error per Pixel')\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reconstruction errors by band (channel)\n",
    "for i in range(reconstruction_error.shape[0]):\n",
    "    plt.imshow(reconstruction_error[i], cmap='hot')\n",
    "    plt.colorbar(label='Reconstruction Error')\n",
    "    wavelength = wlens[i]\n",
    "    plt.title(f'Reconstruction Error for wavelength {wavelength} nm')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
