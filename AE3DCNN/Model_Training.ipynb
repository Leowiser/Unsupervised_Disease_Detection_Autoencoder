{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "735b6308",
   "metadata": {},
   "source": [
    "# Model Training pipeling\n",
    "\n",
    "First load all necessary libraries and the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5feb6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import torch.optim as optim\n",
    "import numpy as np \n",
    "import joblib\n",
    "import gc\n",
    "from torch.utils.data import DataLoader\n",
    "from preprocessing import *\n",
    "from utils import *\n",
    "from datasets import *\n",
    "from CNN_AE_helper import *\n",
    "from CNN3d import *\n",
    "from torchvision.transforms import v2\n",
    "from scipy.ndimage import binary_erosion\n",
    "import plotly\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7dafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FX10 camera\n",
    "#IMG_DIR = 'C:/Users/leonw/OneDrive - KU Leuven/Master Thesis/Data_cropped/cropped_hdf5/'\n",
    "#IMG_DIR = '/home/u0158953/data/Strawberries/PotsprocessedData/cropped_hdf5'\n",
    "IMG_DIR = '/home/r0979317/Documents/Thesis_Strawberries/Data/cropped_hdf5'\n",
    "CAMERA = 'FX10'\n",
    "\n",
    "# Healthy leaves\n",
    "DATES = ['07SEPT2023', '08SEPT2023', '09SEPT2023', '10SEPT2023', '11SEPT2023', '12SEPT2023',\n",
    "         '13SEPT2023', '14SEPT2023', '15SEPT2023', '18SEPT2023', '19SEPT2023']\n",
    "TRAYS = ['3D', '4C', '4D', '2D']    # Some files from the FX17 camera are mistakenly named in 2D instead of 4D\n",
    "healthy_FX10 = filter_filenames(folder_path=IMG_DIR, camera_id=CAMERA, date_stamps=DATES, tray_ids=TRAYS)\n",
    "\n",
    "# Early diseased leaves\n",
    "DATES = ['07SEPT2023']\n",
    "TRAYS = ['3C']\n",
    "early_diseased_FX10 = filter_filenames(folder_path=IMG_DIR, camera_id=CAMERA, date_stamps=DATES, tray_ids=TRAYS)\n",
    "\n",
    "# Mid diseased leaves\n",
    "DATES = ['08SEPT2023', '09SEPT2023']\n",
    "TRAYS = ['3C']\n",
    "mid_diseased_FX10 = filter_filenames(folder_path=IMG_DIR, camera_id=CAMERA, date_stamps=DATES, tray_ids=TRAYS)\n",
    "\n",
    "# Late diseased leaves\n",
    "DATES = ['10SEPT2023', '11SEPT2023', '12SEPT2023', '13SEPT2023', '14SEPT2023', '15SEPT2023']\n",
    "TRAYS = ['3C']\n",
    "late_diseased_FX10 = filter_filenames(folder_path=IMG_DIR, camera_id=CAMERA, date_stamps=DATES, tray_ids=TRAYS)\n",
    "\n",
    "# Number of samples in each category\n",
    "print(f'Healthy: {len(healthy_FX10)}')\n",
    "print(f'Early diseased: {len(early_diseased_FX10)}')\n",
    "print(f'Mid diseased: {len(mid_diseased_FX10)}')\n",
    "print(f'Late diseased: {len(late_diseased_FX10)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79012bbc",
   "metadata": {},
   "source": [
    "Split data into train, validation, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ecd15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(healthy_FX10, test_size=0.20, random_state=10)\n",
    "train, validation = train_test_split(train, test_size=0.185, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa1384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT_DATA = healthy_FX10[0:10]    # [0:40] just to speed up the process for now\n",
    "#MASK_FOLDER = 'C:/Users/leonw/OneDrive - KU Leuven/Master Thesis/Data_cropped/cropped_masks'\n",
    "#MASK_FOLDER = \"/home/u0158953/data/Strawberries/PotsprocessedData/cropped_masks\"\n",
    "MASK_FOLDER = '/home/r0979317/Documents/Thesis_Strawberries/Data/cropped_masks'\n",
    "BATCH_SIZE = 2\n",
    "MASK_METHOD = 1    # 0 for only leaf, 1 for leaf+stem\n",
    "BAND_SELECTION = [489.3, 505.1, 542.21, 550.2, 558.21, 582.31, 625.4, 660.62, 674.2, 679.64,\n",
    "                  701.44, 717.81, 736.94, 745.15, 783.52, 866.08, 951.83]    # Important wavelengths obtained from pca_bandselect.ipynb # extra if needed: 819.25\n",
    "POLYORDER = 2\n",
    "WINDOW_LENGTH = 4 \n",
    "PREPROCESS_METHOD = \"normal\" # Can be: \"normal\", \"savgol\", \"snv\", \"stacked\"\n",
    "PATCH_PROBABILITY = 0.0 # amount of data the dataloader randomly zooms into\n",
    "SCALER = joblib.load('/home/r0979317/Documents/Thesis_Strawberries/Thesis_code/master_thesis/models/pca/scaler_healthy.joblib')    \n",
    "PCA = joblib.load('/home/r0979317/Documents/Thesis_Strawberries/Thesis_code/master_thesis/models/pca/pca_model_healthy.joblib')  \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "print(f'Total number of GPUs: {torch.cuda.device_count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab7fae",
   "metadata": {},
   "source": [
    "Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d7aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentations\n",
    "train_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize((256, 256)),    # By default this uses bilinear interpolation which is good.\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomRotation(degrees=(0, 180), interpolation=v2.InterpolationMode.BILINEAR),\n",
    "])\n",
    "\n",
    "test_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize((256, 256)),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e78d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset and DataLoader\n",
    "#####################\n",
    "### TRAINING DATA ###\n",
    "#####################\n",
    "dataset_train_hsi = HsiDataset(train, MASK_FOLDER, transform=train_transforms, \n",
    "                               apply_mask=True, mask_method=MASK_METHOD, min_wavelength=430, normalize=True, selected_bands=BAND_SELECTION, \n",
    "                               pca=None, scaler=None, polyorder=POLYORDER, window_length=WINDOW_LENGTH, preprocess_method = PREPROCESS_METHOD, patch_probability = PATCH_PROBABILITY)\n",
    "dataloader_train_hsi = DataLoader(dataset_train_hsi, batch_size=BATCH_SIZE, shuffle=True, collate_fn=None)\n",
    "\n",
    "#######################\n",
    "### VALIDATION DATA ###\n",
    "#######################\n",
    "dataset_validation_hsi = HsiDataset(validation, MASK_FOLDER, transform=test_transforms, \n",
    "                               apply_mask=True, mask_method=MASK_METHOD, min_wavelength=430, normalize=True, selected_bands=BAND_SELECTION, \n",
    "                               pca=None, scaler=None, polyorder=POLYORDER, deriv = 2, window_length=WINDOW_LENGTH, preprocess_method = PREPROCESS_METHOD, patch_probability = PATCH_PROBABILITY)\n",
    "dataloader_validation_hsi = DataLoader(dataset_validation_hsi, batch_size=BATCH_SIZE, shuffle=False, collate_fn=None)\n",
    "\n",
    "#################\n",
    "### TEST DATA ###\n",
    "#################\n",
    "dataset_test_hsi = HsiDataset(test, MASK_FOLDER, transform=test_transforms, \n",
    "                               apply_mask=True, mask_method=MASK_METHOD, min_wavelength=430, normalize=True, selected_bands=BAND_SELECTION, \n",
    "                               pca=None, scaler=None, polyorder=POLYORDER, deriv = 2, window_length=WINDOW_LENGTH, preprocess_method = PREPROCESS_METHOD, patch_probability = PATCH_PROBABILITY)\n",
    "dataloader_test_hsi = DataLoader(dataset_test_hsi, batch_size=BATCH_SIZE, shuffle=False, collate_fn=None)\n",
    "\n",
    "###########################\n",
    "### EARLY DISEASED DATA ###\n",
    "###########################\n",
    "dataset_early_diseased_hsi = HsiDataset(early_diseased_FX10, MASK_FOLDER, transform=test_transforms, \n",
    "                               apply_mask=True, mask_method=MASK_METHOD, min_wavelength=430, normalize=True, selected_bands=BAND_SELECTION, \n",
    "                               pca=None, scaler=None, polyorder=POLYORDER, deriv = 2, window_length=WINDOW_LENGTH, preprocess_method = PREPROCESS_METHOD, patch_probability = PATCH_PROBABILITY)\n",
    "dataloader_early_diseased_hsi = DataLoader(dataset_early_diseased_hsi, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "#########################\n",
    "### MID DISEASED DATA ###\n",
    "#########################\n",
    "dataset_mid_diseased_hsi = HsiDataset(mid_diseased_FX10, MASK_FOLDER, transform=test_transforms, \n",
    "                               apply_mask=True, mask_method=MASK_METHOD, min_wavelength=430, normalize=True, selected_bands=BAND_SELECTION, \n",
    "                               pca=None, scaler=None, polyorder=POLYORDER, deriv = 2, window_length=WINDOW_LENGTH, preprocess_method = PREPROCESS_METHOD, patch_probability = PATCH_PROBABILITY)\n",
    "dataloader_mid_diseased_hsi = DataLoader(dataset_mid_diseased_hsi, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "##########################\n",
    "### LATE DISEASED DATA ###\n",
    "##########################\n",
    "dataset_late_diseased_hsi = HsiDataset(late_diseased_FX10, MASK_FOLDER, transform=test_transforms, \n",
    "                               apply_mask=True, mask_method=MASK_METHOD, min_wavelength=430, normalize=True, selected_bands=BAND_SELECTION, \n",
    "                               pca=None, scaler=None, polyorder=POLYORDER, deriv = 2, window_length=WINDOW_LENGTH, preprocess_method = PREPROCESS_METHOD, patch_probability = PATCH_PROBABILITY)\n",
    "dataloader_late_diseased_hsi = DataLoader(dataset_late_diseased_hsi, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4e287a",
   "metadata": {},
   "source": [
    "# Training process\n",
    "\n",
    "- Training of a full model\n",
    "- Different hyperparameter tuning pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dbb8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "##### Train the model\n",
    "#############################################\n",
    "\n",
    "torch.manual_seed(10)\n",
    "torch.cuda.manual_seed_all(10)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# Instantiate the chosen model with the chosen parameters. These change depending on the architecture\n",
    "# Models can be found in CNN3d.py\n",
    "model = CNN3DAE_Exact_Configurable(\n",
    "        base_channels=[72, 128, 128],\n",
    "        dropout_p=0.06405922911448286,\n",
    "        pool_type=\"avg\",\n",
    "        use_batchnorm=[False, False, False]\n",
    "    ).to(device)\n",
    "\n",
    "# Define loss function. Here it is MAE loss\n",
    "criterion = torch.nn.L1Loss()\n",
    "\n",
    "# Define Optimizer with the according learning rate and weight decay\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5.3302310878295036e-05, weight_decay=8.389079019851564e-05)\n",
    "\n",
    "\n",
    "# Train the model using the function from CNN_AE_helper.py\n",
    "train_losses, val_losses = train_autoencoder(\n",
    "    75,\n",
    "    model,\n",
    "    dataloader_train_hsi,\n",
    "    device,\n",
    "    dataloader_validation_hsi,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    save_model=True,\n",
    "    save_path='/home/r0979317/Documents/Thesis_Strawberries/models/third_finetuning_model.pth', noise = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb8529f",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13cd104-f586-4f89-a82a-6267956df380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#### First hyperparameter tuning set up\n",
    "#############################################\n",
    "\n",
    "def objective(trial, train = dataloader_train_hsi, validation = dataloader_validation_hsi):\n",
    "    \n",
    "    trial_start = time.time()\n",
    "    \n",
    "    # === Number of Layers ===\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 4, 6)\n",
    "    \n",
    "    layers_list = []\n",
    "    current_min = 8\n",
    "    for i in range(n_layers):\n",
    "        ch = trial.suggest_int(f\"enc_ch_{i}\", current_min, 256, step=8)\n",
    "        layers_list.append(ch)\n",
    "        if ch < 248:\n",
    "            current_min = ch + 8  # ensure next layer has strictly larger channels\n",
    "        else:\n",
    "            current_min = ch\n",
    "        \n",
    "    \n",
    "    # Optional: prune if model is too large since I run out of Memory often\n",
    "    if sum(layers_list) > 900:  \n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # === Dropout, LR, etc. ===\n",
    "    #bdropout_p = trial.suggest_float(\"dropout\", 0.2, 0.4)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 7e-3, log=True)\n",
    "    #optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\"]) # Initial fine tuning found RMSprop to perform worse\n",
    "    # Suggest criterion type\n",
    "    criterion_name = trial.suggest_categorical(\"criterion\", [\"MSE\", \"MAE\"])\n",
    "    \n",
    "    \n",
    "    print(f'Model {trial.number} with Layers: {layers_list}, LR: {lr:.1e}, WD: {weight_decay:.1e}, Loss criterion: {criterion_name}')\n",
    "\n",
    "    # === Model ===\n",
    "    model = CNN3DAEMAX_try(\n",
    "        layers_list=layers_list,\n",
    "        input_dim=1,\n",
    "        kernel_sizes=3,\n",
    "        strides=(1, 2, 2),\n",
    "        paddings=1).to(device)\n",
    "\n",
    "    # === Optimizer ===\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    #else:\n",
    "    #    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    if criterion_name == \"MSE\":\n",
    "        criterion = nn.MSELoss()\n",
    "    elif criterion_name == \"MAE\":\n",
    "        criterion = nn.L1Loss()\n",
    "\n",
    "    train_losses, val_losses = train_autoencoder(\n",
    "        num_epochs=20,\n",
    "        model=model,\n",
    "        dataloader_train=train,\n",
    "        device=device,\n",
    "        dataloader_valid=validation,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        save_model=False\n",
    "    )\n",
    "\n",
    "    best_val_loss = min(val_losses)\n",
    "    \n",
    "    # === Logging ===\n",
    "    elapsed = (time.time() - trial_start) / 60\n",
    "    print(f'Trial {trial.number} finished in {elapsed:2f} min')\n",
    "    print(f'Minimal val loss: {best_val_loss:.6f}')\n",
    "    \n",
    "    # === Cleanup ===\n",
    "    del model, optimizer, criterion, train_losses, val_losses\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    time.sleep(1)\n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614c4ca9-1f3e-498f-b368-b9ae49a299ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#### Second hyperparameter tuning set up\n",
    "#############################################\n",
    "def objective_finetuning(trial, train = dataloader_train_hsi, validation = dataloader_validation_hsi):\n",
    "\n",
    "    trial_start = time.time()\n",
    "    # === Number of Layers ===\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 6)\n",
    "\n",
    "    # === Dynamically define channels per layer ===\n",
    "    layers_list = []\n",
    "    current_min = 8\n",
    "    for i in range(n_layers):\n",
    "        ch = trial.suggest_int(f\"enc_ch_{i}\", current_min, 256, step=8)\n",
    "        layers_list.append(ch)\n",
    "        if ch < 248:\n",
    "            current_min = ch + 8  # ensure next layer has strictly larger channels\n",
    "        else:\n",
    "            current_min = ch\n",
    "        \n",
    "    \n",
    "    # Optional: prune if model is too large since I run out of Memory often\n",
    "    if sum(layers_list) > 900:  \n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # === Dropout, LR, etc. ===\n",
    "    dropout_p = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    # ===== Activation choice =====\n",
    "    act_name = trial.suggest_categorical(\"activation\", [\"relu\", \"gelu\", \"elu\"])\n",
    "\n",
    "    # ——— 1) Loss function choice ———\n",
    "    loss_name = trial.suggest_categorical(\"loss_fn\", [\"MAE\", \"MSE\"])\n",
    "    if loss_name == \"MAE\":\n",
    "        loss_fn = torch.nn.L1Loss()\n",
    "    else:\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "\n",
    "    # ——— 2) BatchNorm / Pooling choices ———\n",
    "    use_bn_flag = trial.suggest_categorical(\"use_bn\", [False, True])\n",
    "    use_batchnorm = [use_bn_flag] * n_layers\n",
    "    \n",
    "    # ===== Global pooling choice =====\n",
    "    # 1) you always sample from the same three options\n",
    "    pool_type = trial.suggest_categorical(\"pool_type\", [\"none\",\"max\",\"avg\"])\n",
    "    \n",
    "    # 2) but if you have too many layers, that combo is invalid—\n",
    "    #    prune it right away so it never skews your results\n",
    "    if len(layers_list) > 4 and pool_type != \"none\":\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    print(f'Model {trial.number} with Layers: {layers_list}, LR: {lr:.1e}, WD: {weight_decay:.1e}, Loss criterion: {loss_name}, Pooling: {pool_type}, BatchNorm: {use_bn_flag}, activation: {act_name}')\n",
    "\n",
    "    strides = [(1,2,2)] * len(layers_list)\n",
    "\n",
    "    # ===== instantiate model =====\n",
    "    model = CNN3DAE_finetuning(\n",
    "        layers_list=layers_list,\n",
    "        strides=strides,\n",
    "        dropout_p=dropout_p,\n",
    "        pool_type=pool_type,\n",
    "        use_batchnorm=use_batchnorm,\n",
    "        activation=act_name,         # new argument\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    # === Optimizer ===\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # FIXED validation metric\n",
    "    train_losses, val_losses = train_autoencoder(\n",
    "        num_epochs=20,\n",
    "        model=model,\n",
    "        dataloader_train=train,\n",
    "        device=device,\n",
    "        dataloader_valid=validation,\n",
    "        criterion=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        save_model=False\n",
    "    )\n",
    "\n",
    "    best_val_loss = min(val_losses)\n",
    "    \n",
    "    # === Logging ===\n",
    "    elapsed = (time.time() - trial_start) / 60\n",
    "    print(f'Trial {trial.number} finished in {elapsed:2f} min')\n",
    "    print(f'Minimal val loss: {best_val_loss:.6f}')\n",
    "    \n",
    "    # === Cleanup ===\n",
    "    try:\n",
    "        del model, optimizer, loss_fn, train_losses, val_losses\n",
    "    except NameError:\n",
    "        # in case we pruned early and some of these don't exist\n",
    "        pass\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    time.sleep(1)\n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026881c8",
   "metadata": {},
   "source": [
    "Check exact upsampling of the model without interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96c8a6-c0cd-4df3-82d4-4234f96d19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN3DAE_Exact_Configurable(\n",
    "    input_dim=1,\n",
    "    base_channels=[8,16,32, 64, 128],\n",
    "    dropout_p=0.2,\n",
    "    pool_type=\"avg\",              # or \"avg\", or \"none\"\n",
    "    use_batchnorm=[False, True, True, True, True]\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 1, 18, 256, 256)\n",
    "out = model(x)\n",
    "print(out.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff353653-4d3d-46be-bc7a-05875cc7dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#### Third hyperparameter tuning set up\n",
    "#############################################\n",
    "def objective_checkerboard(trial,\n",
    "                           train=dataloader_train_hsi,\n",
    "                           validation=dataloader_validation_hsi):\n",
    "\n",
    "    trial_start = time.time()\n",
    "    # Number of Layers\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 3, 5)\n",
    "\n",
    "    #  Dynamically define channels per layer, force them to increase size\n",
    "    layers_list = []\n",
    "    current_min = 8\n",
    "    for i in range(n_layers):\n",
    "        ch = trial.suggest_int(f\"enc_ch_{i}\", current_min, 128, step=8)\n",
    "        layers_list.append(ch)\n",
    "        current_min = ch + 8 if ch < 120 else ch\n",
    "\n",
    "    # Prune overly large models so we don't run out of CUDA memory\n",
    "    if sum(layers_list) > 600:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # Dropout, LR, weight decay\n",
    "    dropout_p    = trial.suggest_float(\"dropout\",      0.0, 0.5)\n",
    "    lr           = trial.suggest_float(\"lr\",           1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    # Pooling choice \n",
    "    pool_type = trial.suggest_categorical(\"pool_type\", [\"none\", \"max\", \"avg\"])\n",
    "    \n",
    "    # Per-layer Batch normalization\n",
    "    use_batchnorm = []\n",
    "    for i in range(n_layers):\n",
    "        flag = trial.suggest_categorical(f\"use_bn_layer_{i}\", [False, True])\n",
    "        use_batchnorm.append(flag)\n",
    "\n",
    "\n",
    "    print(\n",
    "        f\"Trial {trial.number}: layers={layers_list}, \"\n",
    "        f\"lr={lr:.1e}, wd={weight_decay:.1e}, \"\n",
    "        f\"pool={pool_type}, bn={use_batchnorm}\"\n",
    "    )\n",
    "\n",
    "    # Build model\n",
    "    model = CNN3DAE_Exact_Configurable(\n",
    "        input_dim=1,\n",
    "        base_channels=layers_list,\n",
    "        dropout_p=dropout_p,\n",
    "        pool_type=pool_type,\n",
    "        use_batchnorm=use_batchnorm,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "    train_losses, val_losses = train_autoencoder(\n",
    "        num_epochs=20,\n",
    "        model=model,\n",
    "        dataloader_train=train,\n",
    "        device=device,\n",
    "        dataloader_valid=validation,\n",
    "        criterion=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        save_model=False,\n",
    "        trial=trial\n",
    "    )\n",
    "\n",
    "    best_val_loss = min(val_losses)\n",
    "    elapsed = (time.time() - trial_start) / 60\n",
    "    print(f\"Trial {trial.number} finished in {elapsed:.2f} min — best val loss {best_val_loss:.6f}\")\n",
    "\n",
    "    # cleanup\n",
    "    del model, optimizer, loss_fn, train_losses, val_losses\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    time.sleep(1)\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a25f9a9-21b2-4433-a38b-0dbcb16c4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define study_path as filen_name.db\n",
    "study_path = \"/home/r0979317/Documents/Thesis_Strawberries/...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aca390-1204-4e52-aa8b-6010e50f4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = optuna.pruners.PercentilePruner(\n",
    "    percentile=50.0,         # Only keep top 50% of trials\n",
    "    n_startup_trials=6,      # Don't prune the first 6 trials\n",
    "    n_warmup_steps=2,        # Start checking after the second epoch\n",
    "    interval_steps=1,        # Check every epoch\n",
    "    n_min_trials=1           # Minimum # of trials to start pruning comparisons\n",
    ")\n",
    "\n",
    "# Or use other pruner for run 1 and 2\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner()\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    pruner=pruner,\n",
    "    study_name=\"...\",       # Here the name of the study has to be defined\n",
    "    storage=f\"sqlite:///{study_path}\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "\n",
    "study.optimize(objective_checkerboard, n_trials=100, timeout=40000, gc_after_trial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25a0e6-4a79-4922-b772-e10803f300bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the study to visualize it\n",
    "study_test = optuna.load_study(\n",
    "    study_name= \"...\",#\"zoomed_finetuning_study\",\n",
    "    storage=f\"sqlite:///{study_path}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58f15b-9d68-4304-9383-15f7a3d422c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best trial and its parameters\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value (objective score): {best_trial.value}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79029c77-bad8-466e-ade3-5a5ad33947f4",
   "metadata": {},
   "source": [
    "Visualize hyperparameter tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de9c6e-ceb8-492a-bd73-655d18b9fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pio.renderers.default = \"notebook\"  # or \"iframe\" if this fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b9309-9811-45e2-8542-60be952e9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort all completed trials by objective value (lowest = best)\n",
    "completed_trials = [t for t in study_test.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "top5 = sorted(completed_trials, key=lambda t: t.value)[:9]\n",
    "\n",
    "# Print them\n",
    "for i, trial in enumerate(top5):\n",
    "    print(f\"\\nRank {i+1}:\")\n",
    "    print(f\"  Trial number: {trial.number}\")\n",
    "    print(f\"  Objective value: {trial.value:.6f}\")\n",
    "    print(f\"  Params: {trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b0b9f0-3c53-4936-a52b-ee67b9ed914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_optimization_history(study).show()\n",
    "vis.plot_param_importances(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b253d6-e5a4-4668-a816-b545016e8f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_parallel_coordinate(study_test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e486f57",
   "metadata": {},
   "source": [
    "### Loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture and optimizer again\n",
    "model = CNN3DAE_TightDropout(\n",
    "        layers_list=[18, 32, 64, 64],\n",
    "        input_dim=1,\n",
    "        strides=(1, 2, 2),\n",
    "        paddings=(0, 1, 1)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# Load the checkpoint\n",
    "model, optimizer, train_losses, val_losses, last_epoch = load_checkpoint(model, '/home/r0979317/Documents/Thesis_Strawberries/models/first_pc_dropout_model.pth', device, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_thesis)",
   "language": "python",
   "name": "env_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
